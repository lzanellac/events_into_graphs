{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Graphs_of_events_sentences.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOw9xRm5jKry1xCgqKKQsUj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lzanellac/events_into_graphs/blob/main/Graphs_of_events_sentences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo0GDiLOb8Mq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "754385e2-e237-4b20-9689-51d47e24c9f0"
      },
      "source": [
        "import os\n",
        "import networkx as nx\n",
        "import holoviews as hv\n",
        "import glob\n",
        "import nltk\n",
        "import spacy\n",
        "import ast\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "nltk.download('punkt')\n",
        "#!python -m spacy download en_core_web_md"
      ],
      "execution_count": 450,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 450
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jmvo-h0pfU18"
      },
      "source": [
        "# Reading files from directory .txt and .ann\n",
        "def files_(directory): \n",
        "  files = []\n",
        "  for filename in os.listdir(directory):\n",
        "    filename = filename.split(\".\")[0]\n",
        "    files.append(filename)\n",
        "    files = list(dict.fromkeys(files))\n",
        "  print(files)\n",
        "  return files # files returns a list with the files in the directory (not repeated)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TICGnNVfZaR"
      },
      "source": [
        "# Reformating the files \n",
        "def reformat_events(lines):\n",
        "  events, num_nod, listevents = [], [], []\n",
        "  for line in lines:\n",
        "    if line.startswith('E'):\n",
        "      events.append(line)\n",
        "  for ev in range(len(events)):\n",
        "    types, ids, entities, roles = [], [], [], []\n",
        "    id_event, event = events[ev].split('\\t')[0], events[ev].split('\\t')[1]\n",
        "    args_split = event.split(' ')\n",
        "    for arg in range(len(args_split)):\n",
        "      roles.append(args_split[arg].split(':')[0])\n",
        "      ids.append(args_split[arg].split(':')[1])\n",
        "      for line1 in lines:\n",
        "        if line1.startswith(args_split[arg].split(':')[1]+'\\t'):\n",
        "          if line1.startswith('E'): \n",
        "            for line in lines:\n",
        "              if line.startswith(ids[arg]):\n",
        "                id = line.split(':')[1] \n",
        "            for line in lines:\n",
        "              if line.startswith(id.split(\" \")[0]):\n",
        "                entities.append(line.split('\\t')[2])\n",
        "                type_ = line.split('\\t')[1]\n",
        "                types.append(type_.split(' ')[0])\n",
        "          else:\n",
        "            type_ = line1.split('\\t')[1]\n",
        "            types.append(type_.split(' ')[0])\n",
        "            entities.append(line1.split('\\t')[2])\n",
        "    num_nod.append(len(types))\n",
        "    listevents.append((ids, types, entities, roles))\n",
        "  #print(listevents)\n",
        "  \n",
        "  return listevents, num_nod\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px867vwwfeZR"
      },
      "source": [
        "#Add multiple nodes with node level attributes and edges with edge level attributes\n",
        "def graph(listevents, num_nod, PMID):\n",
        "  # Create a directed graph G\n",
        "  G = nx.DiGraph()\n",
        "  G.graph[\"Content\"] = \"Events\"\n",
        "\n",
        "  #entities_per_sentence = nodes_sentence(PMID)\n",
        "\n",
        "  # Add multiple nodes with node level attributes and edges with edge level attributes\n",
        "  nodes, cur_node, triggers = 0, 0, []\n",
        "  for event in range(len(listevents)):\n",
        "    for ev in range(len(listevents[event][0])):\n",
        "        G.add_nodes_from([(nodes, {'ent_id' : listevents[event][0][ev], 'type' : listevents[event][1][ev], 'entity' : listevents[event][2][ev]})])\n",
        "        if ev == 0:\n",
        "          triggers.append(nodes)\n",
        "        if nodes < sum(num_nod):\n",
        "          nodes += 1\n",
        "    nodes_list = list(G.nodes)\n",
        "    cur_node = triggers[event]\n",
        "    for nod in range(len(listevents[event][0])-1):\n",
        "      G.add_edges_from([(triggers[event], nodes_list[cur_node+1], {\"role\" : listevents[event][3][nod+1]})])\n",
        "      cur_node += 1\n",
        "  \n",
        "  return G"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61GDT1XcfkAV"
      },
      "source": [
        "# Visualization\n",
        "def vis_graph(G, pmid):\n",
        "  hv.extension('bokeh')\n",
        "  graph = hv.Graph.from_networkx(G, nx.layout.fruchterman_reingold_layout).opts(tools=['hover'], directed=True, arrowhead_length=0.04, node_color='type', cmap=['lightsteelblue','steelblue'],\n",
        "                                                                              node_size=40, edge_hover_line_color='black' , node_hover_fill_color='lightseagreen',\n",
        "                                                                              edge_color='lightgray', width=700, height=600, title = pmid)\n",
        "  labels_node = hv.Labels(graph.nodes, ['x', 'y'], 'entity')\n",
        "  return (graph * labels_node.opts(text_font_size='10pt', text_color='black'))"
      ],
      "execution_count": 445,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGqoRNMjkY53"
      },
      "source": [
        "# Tokenization\n",
        "def token(PMID):\n",
        "  f = open(PMID + '.txt', \"r\")\n",
        "  f = f.read()\n",
        "  sentences = nltk.tokenize.sent_tokenize(f)\n",
        "  return sentences\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPaabl_kmJmI"
      },
      "source": [
        "# Look for the entities and their ids that are contained in each sentence\n",
        "def nodes_sentence(PMID): \n",
        "  nlp = spacy.load('en_core_web_sm')\n",
        "  sentences, entities, ids_per_sentence = [], [], []\n",
        "  offset_i, offset_o = 0, 0\n",
        "  d, ids = {}, {}\n",
        "\n",
        "  f = open(PMID + '.txt', \"r\")\n",
        "  f = f.read()\n",
        "  doc = nlp(f)\n",
        "  #sentence = nltk.tokenize.sent_tokenize(f)\n",
        "\n",
        "  with open(PMID + '.ann') as file:\n",
        "        lines = file.readlines()\n",
        "        lines = [line.rstrip() for line in lines]\n",
        "\n",
        "  for line in lines:\n",
        "    if line.startswith('T'):\n",
        "      entities.append(line)\n",
        "  # Look for the sentences containing the entities in .ann\n",
        "  for ent in range(len(entities)):\n",
        "    offset_i, offset_o = (entities[ent].split('\\t')[1]).split(' ')[1:]\n",
        "    word = doc.char_span(int(offset_i), int(offset_o))\n",
        "    idx = entities[ent].split('\\t')[0]\n",
        "    if word is not None:\n",
        "      sent = word.sent\n",
        "      #print(sent)\n",
        "      sentences.append((sent, word, idx)) # sentences contains the sentence + entity contained + idx of the entity (sentences are repeated for each entity)\n",
        "  # Merging sublists to have all the entities contained in the sentence in the same sublist [(sentence), entities]\n",
        "  for k, v, idx in sentences:\n",
        "    d.setdefault(k, [k]).append((idx, v))\n",
        "  b = (tuple, d.values())\n",
        "  entities_per_sentence = list(b[1]) # entities_per_sentence contains list of sentence + all entities contained [(sentence), entities]\n",
        "\n",
        "  for i in range(len(entities_per_sentence)):\n",
        "    ids_per_sentence.append([a_tuple[0] for a_tuple in entities_per_sentence[i][1:]]) # ids_per_sentence contains a list of lists of ids of the entities contained per sentence\n",
        "  \n",
        "  return entities_per_sentence, ids_per_sentence # entities_per_sentence contains a list of sublists [[sentence], (id, word_entity)], ids_per_sentence contains a list of sublists with entities ids per sentence "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dF0ARCvyX5p"
      },
      "source": [
        "# Merge lists that contain the same first element (autorship from stackoverflow)\n",
        "def merge_subs(lst_of_lsts):\n",
        "    res = []\n",
        "    for row in lst_of_lsts:\n",
        "        for i, resrow in enumerate(res):\n",
        "            if row[0]==resrow[0]:\n",
        "                res[i] += row[1:]\n",
        "                break\n",
        "        else:\n",
        "            res.append(row)\n",
        "    return res"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBCMOCC1gJbC"
      },
      "source": [
        "# Look for the lines in .ann that correspond to the entities in the sentence (given by ids_per_sentence)\n",
        "def nodes_per_sentence(ids_per_sentence, lines):\n",
        "  entities, lin_per_sentence, line_per_sentence = [], [], []\n",
        "\n",
        "  for line in lines:\n",
        "    if line.startswith('T'):\n",
        "      entities.append(line)\n",
        "\n",
        "  for i in range(len(ids_per_sentence)):\n",
        "    for j in range(len(ids_per_sentence[i])):\n",
        "      for h in range(len(entities)):\n",
        "        if entities[h].startswith(ids_per_sentence[i][j]+'\\t'):\n",
        "          lin_per_sentence.append(entities[h])\n",
        "    #print(lin_per_sentence)    \n",
        "    line_per_sentence.append(lin_per_sentence)\n",
        "    lin_per_sentence = []\n",
        "\n",
        "  return line_per_sentence #line_per_sentence contains a list of lists of the lines in .ann that correspond to the all entities in the sentence "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WdiQkts5EMk"
      },
      "source": [
        "# Finds all triggers contained among the entities of each sentence (if the sentence doesn't contain triggers, it is left out)\n",
        "def trig_sentence(line_per_sentence, trigger_types): \n",
        "  trig_per_sentence, triggers_per_sentence = [], []\n",
        "  for ents in range(len(line_per_sentence)):\n",
        "    for ent in range(len(line_per_sentence[ents])):\n",
        "      if (line_per_sentence[ents][ent].split('\\t')[1]).split(' ')[0] in trigger_types:\n",
        "        trig_per_sentence.append((line_per_sentence[ents],line_per_sentence[ents][ent].split('\\t')[0]))\n",
        "  triggers_per_sentence = merge_subs(trig_per_sentence) # size(triggers_per_sentence)\n",
        "  return triggers_per_sentence # triggers_per_sentence contains a list of sublists with [([entities per sentence], all triggers in the entities)]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCjYDU7umh9O"
      },
      "source": [
        "# Read the file with the dictionary containing the pre-defined trigger types of the model\n",
        "def list_triggers(triggers_file):\n",
        "  triggers_file = open(triggers_file, 'r')\n",
        "  content = triggers_file.read()\n",
        "  trigger_dict = ast.literal_eval(content)\n",
        "  triggers_file.close()\n",
        "  trigger_types = trigger_dict['trigger_types']\n",
        "  return trigger_types # trigger_types contains a list of the pre-defined trigger types of the model"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucMBlhVdBRsv"
      },
      "source": [
        "# Finds events corresponding to each sentence\n",
        "def event_per_sentence(triggers_per_sentence, list_events):\n",
        "  ev_per_sentence, evs_per_sentence, evs_per_abstract = [], [], []\n",
        "  for abstract in range(len(triggers_per_sentence)):\n",
        "    for sentence in range(len(triggers_per_sentence[abstract])):\n",
        "      for event in range(len(list_events)):\n",
        "        if list_events[event][0][0] in list(triggers_per_sentence[abstract][sentence][1:]):\n",
        "          #print('sentence = ', sentence)#, ' ',list_events[event][0][0], list_events[event][0], '', list(triggers_per_sentence[abstract][sentence][1:]))\n",
        "          ev_per_sentence.append(list_events[event][0])\n",
        "          #print(ev_per_sentence)\n",
        "      if ev_per_sentence != []:\n",
        "        evs_per_sentence.append((list(triggers_per_sentence[abstract][sentence][1:]), ev_per_sentence))\n",
        "        #print('evs_per_sentence = ' , evs_per_sentence)\n",
        "        if evs_per_sentence != []:\n",
        "          events_per_sentence.append(evs_per_sentence)\n",
        "          print(events_per_sentence)\n",
        "      ev_per_sentence, evs_per_sentence = [], []\n",
        "  #print('events_per_abstract = ', evs_per_abstract)\n",
        "  return evs_per_abstract, evs_per_sentence # return a list of the events per abstract and a list of events per sentence\n"
      ],
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-lAzrq_-B-1"
      },
      "source": [
        "# Reads triggers_per_sentence ([entities], triggers) and reformat them for the creation of graphs\n",
        "def reformat_sent(triggers_per_sentence):\n",
        "  id_ent, type_ent, entity, format_sent, num_nod_sent = [], [], [], [], []\n",
        "  for i in range(len(triggers_per_sentence)):\n",
        "    for j in range(len(triggers_per_sentence[i][0])):\n",
        "      id_ent.append(triggers_per_sentence[i][0][j].split('\\t')[0])\n",
        "      #print(id_ent)\n",
        "      type_ent.append((triggers_per_sentence[i][0][j].split('\\t')[1]).split(' ')[0])\n",
        "      entity.append(triggers_per_sentence[i][0][j].split('\\t')[2])\n",
        "    num_nod_sent.append(len(id_ent))\n",
        "    format_sent.append((id_ent, type_ent, entity))\n",
        "    id_ent, type_ent, entity = [], [], []\n",
        "  return format_sent, num_nod_sent # format_sent returns a list of nodes per sentence [id_ent, type_ent, entity] and num_nod_sent returns a list with the # of nodes in the sentence"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19gx-60RFD6c"
      },
      "source": [
        "#Add multiple nodes with node level attributes and edges with edge level attributes\n",
        "def graph_sent(format_sent, num_nod_sent):\n",
        "  # Create a directed graph G\n",
        "  G = nx.DiGraph()\n",
        "  G.graph[\"Content\"] = \"Events\"\n",
        "  graphs = []\n",
        "\n",
        "  # Add multiple nodes with node level attributes and edges with edge level attributes\n",
        "  nodes, cur_node, triggers, nodes_events = 0, 0, [], []\n",
        "  for sentence in range(len(format_sent)):\n",
        "    for sents in range(len(format_sent[sentence])):\n",
        "      for sent in range(len(format_sent[sentence][0])):\n",
        "        G.add_nodes_from([(nodes, {'ent_id' : format_sent[sentence][0][sent], 'type' : format_sent[sentence][1][sent], 'entity' : format_sent[sentence][2][sent]})])  \n",
        "        if nodes < sum(num_nod_sent):\n",
        "          nodes += 1\n",
        "      nodes, cur_node = 0, 0\n",
        "\n",
        "    graphs.append(G)\n",
        "    G = nx.DiGraph()\n",
        "    G.graph[\"Content\"] = \"Events\"\n",
        "  return graphs"
      ],
      "execution_count": 418,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhR-ZEaG6spU"
      },
      "source": [
        "def edges_sentence(graphs, list_events):\n",
        "  nodes_id, nodes, nodes_events = [], [], []\n",
        "  for i in range(len(graphs)): # abstract level\n",
        "    for j in range(len(graphs[i])):\n",
        "      g = graphs[i].nodes()\n",
        "      nodes.append(g[j]['ent_id'])\n",
        "      nodes_id = np.array(list(graphs[i].nodes()))      \n",
        "    for h in range(len(list_events)): \n",
        "      #### code from geeksforgeeks.org\n",
        "      flag = 0\n",
        "      if (all(x in nodes for x in list_events[h][0])):\n",
        "        flag = 1   \n",
        "      if (flag) :\n",
        "      ####\n",
        "        event = list_events[h][0]\n",
        "        for s in range(len(event)):\n",
        "          for p in range(len(nodes)):\n",
        "            if event[s] == nodes[p]:\n",
        "              nodes_events.append(p) # nodes_ids of the events of the sentence\n",
        "        for nod in range(len(nodes_events)-1):\n",
        "          graphs[i].add_edges_from([(nodes_events[0], nodes_events[nod+1], {\"role\" : list_events[h][3][nod+1]})])\n",
        "        #for edge in graphs[i].edges(data=True):\n",
        "        #  print(edge)\n",
        "        nodes_events = [] \n",
        "    nodes = []\n",
        "    print(graphs[i])\n",
        "  return graphs\n"
      ],
      "execution_count": 525,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_RSQRouXg8J"
      },
      "source": [
        " def static_vis(graphs, PMID):\n",
        "   for i in range(len(graphs)):\n",
        "    edge_labels = nx.get_edge_attributes(graphs[i], \"role\")\n",
        "    node_lab = nx.get_node_attributes(graphs[i], \"entity\")\n",
        "    pos = nx.spring_layout(graphs[i], k = 0.2)\n",
        "    fig, ax = plt.subplots(figsize=(12,12))\n",
        "    ax.set_title(PMID)\n",
        "    ax.tick_params(left=True, bottom=True, labelleft=True, labelbottom=True)\n",
        "    nx.draw(graphs[i], pos, labels=node_lab, node_size=1500, node_color=\"steelblue\", edgecolors='black', arrows = True, ax = ax)\n",
        "    nx.draw_networkx_edges(graphs[i], pos=pos)\n",
        "    nx.draw_networkx_edge_labels(graphs[i], pos, edge_labels) \n",
        "    limits=plt.axis('on') # turns on axis\n",
        "    ax.tick_params(left=True, bottom=True, labelleft=True, labelbottom=True)\n",
        "\n",
        "    "
      ],
      "execution_count": 557,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWQE-nHDNmeF",
        "outputId": "a1b937ab-66e0-4a7b-daa2-af0c05ed8888"
      },
      "source": [
        "PMID = 'pmid/PMID-26986801'\n",
        "triggers_file = \"cg.types\"\n",
        "trigger_types = list_triggers(triggers_file)\n",
        "\n",
        "with open(PMID + '.ann') as file:\n",
        "      lines = file.readlines()\n",
        "      lines = [line.rstrip() for line in lines] # lines returns a list of the lines of each abstract\n",
        "    \n",
        "entities_per_sentence, ids_per_sentence = nodes_sentence(PMID) # get the entities and ids that correspond to each sentence\n",
        "lines_per_sentence = nodes_per_sentence(ids_per_sentence, lines) # get the lines of the .ann that correspond to each sentence of the abstract\n",
        "triggers_per_sentence = trig_sentence(lines_per_sentence, trigger_types)\n",
        "list_events, num_nod = reformat_events(lines)\n",
        "format_sent, num_nod_sent = reformat_sent(triggers_per_sentence)\n",
        "graphs = graph_sent(format_sent, num_nod_sent)\n",
        "graphs = edges_sentence(graph_sent(format_sent, num_nod_sent), list_events)\n",
        "print(graphs[0].nodes(data=True))"
      ],
      "execution_count": 547,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DiGraph with 3 nodes and 2 edges\n",
            "DiGraph with 4 nodes and 0 edges\n",
            "DiGraph with 7 nodes and 2 edges\n",
            "DiGraph with 3 nodes and 0 edges\n",
            "DiGraph with 7 nodes and 3 edges\n",
            "[(0, {'ent_id': 'T1', 'type': 'Simple_chemical', 'entity': 'Gallate'}), (1, {'ent_id': 'T2', 'type': 'Simple_chemical', 'entity': 'Gallate Derivatives'}), (2, {'ent_id': 'T32', 'type': 'Planned_process', 'entity': 'Encapsulation'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8jKIxFLCVT3"
      },
      "source": [
        "#graphs = graph_sent(format_sent, num_nod_sent)\n",
        "graphs = edges_sentence(graph_sent(format_sent, num_nod_sent), list_events)\n",
        "print(graphs[0])\n",
        "vis_graph(graphs[0], 'PMID-26986801')\n",
        "static_vis(graphs, PMID)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owA2XzgbfoMl"
      },
      "source": [
        "def main():\n",
        "  # Iteration over the files in the directory\n",
        "  files = files_(path)\n",
        "  ev_abstract, num_nod_abstract, graphs, sentences, triggers_per_sentence = [], [], [], [], []\n",
        "  trigger_types = list_triggers(triggers_file) #get the list of triggers from the cg.types files\n",
        "  for filess in range(len(glob.glob1(path,\"*.ann\"))):\n",
        "  #for filess in range(len(files)):\n",
        "    pmid = files[filess] \n",
        "    PMID = os.path.join(path, pmid)\n",
        "    print(PMID)\n",
        "\n",
        "    #sentences.append(token(PMID))\n",
        "    #print(sentences)\n",
        "\n",
        "    with open(PMID + '.ann') as file:\n",
        "      lines = file.readlines()\n",
        "      lines = [line.rstrip() for line in lines] # lines returns a list of the lines of each abstract\n",
        "    \n",
        "    entities_per_sentence, ids_per_sentence = nodes_sentence(PMID) # get the entities and ids that correspond to each sentence\n",
        "    lines_per_sentence = nodes_per_sentence(ids_per_sentence, lines) # get the lines of the .ann that correspond to each sentence of the abstract\n",
        "    triggers_per_sentence = trig_sentence(lines_per_sentence, trigger_types) # get a list of lists of the ([lines_entities] + id_triggers) contained in each sentence contained in each abstract\n",
        "    # Changing the format of files to be used for the graphs\n",
        "    list_events, num_nod = reformat_events(lines) # list of events per abstract\n",
        "    '''\n",
        "    evs_per_abstract, evs_per_sentence = event_per_sentence(triggers_per_sentence, list_events)\n",
        "    '''\n",
        "    # Changing the format of lines per sentence to be used for the graphs\n",
        "    format_sent, num_nod_sent = reformat_sent(triggers_per_sentence)\n",
        "    print(format_sent)\n",
        "    print(len(format_sent))\n",
        "    print(num_nod_sent)\n",
        "    # Construction of graphs\n",
        "    graphs = graph_sent(format_sent, num_nod_sent) # returns a list with the graphs corresponding to each abstract\n",
        "    graphs = edges_sentence(graph_sent(format_sent, num_nod_sent), list_events)\n",
        "    print(graphs)\n",
        "    for gr in range(len(graphs)): \n",
        "      H = vis_graph(graphs[gr], pmid)\n",
        "      H = H + H\n",
        "    #graphs = edges_sentence(graph_sent(format_sent, num_nod_sent), list_events)\n",
        "    static_vis(graphs, pmid)\n",
        "  \n",
        "  return graphs"
      ],
      "execution_count": 549,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YVl1O7xfydH"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  #path = '/home/lzanella/orpailleur/lzanella/biomolecules-project/Information_extraction/Event_extraction/Graphs/abstracts'\n",
        "  path = \"pmid\" # path to the .ann and .txt files\n",
        "  triggers_file = \"cg.types\" # path to the file with the triggers types\n",
        "  graphs = main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}